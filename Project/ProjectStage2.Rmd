---
title: "Project02"
author: "Max Selvaggio"
date: "2022-07-31"
output:
 html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
library(pROC)
library(rpart)
library(rpart.plot)
library(gamlr)
library(e1071)
```


```{r}
setwd("C:\\Users\\mselv\\Documents\\GitHub\\ECO5445\\Project\\Data")
Mdata <- read.csv("hmda_sw.csv")

setnames(Mdata, old = c('s13','s46','s27a','s23a','school','s17','s7'), 
         new = c('Applicant Race','Debt to Income Ratio','Self-Employed','Marital Status',
                 'Years of Education','Applicant Income','Action Taken'))

Mdata[Mdata == 999999.4] <- NA
Mdata$`Action Taken`[Mdata$`Action Taken` == 2] <- 1
Mdata$`Action Taken` <- factor(Mdata$`Action Taken`,levels = c(1,3), labels = c("Approved","Denied"))

Mdata["Applicant Race"][Mdata["Applicant Race"] == "3"] <- "Black"
Mdata["Applicant Race"][Mdata["Applicant Race"] == "5"] <- "White"

Mdata["Self-Employed"][Mdata["Self-Employed"] == "0"] <- "No"
Mdata["Self-Employed"][Mdata["Self-Employed"] == "1"] <- "Yes"

Mdata$`Applicant Income` <- Mdata$`Applicant Income`*1000
```

The variable "Action Taken" is our qualitative dependent because the approval or denial of a mortgage is the output we are attempting to predict with this model. The reason these variables were chosen is because it is important to have "variables correlated with both race's role and creditworthiness"(Page 1, "Mortgage Lending in Boston: Interpreting HMDA Data"). As the paper indicates, previous studies of how race played a part in the mortgage process did not properly include financial variables as well, and therefore the results were skewed. With these variables, we will be able to see race's affect as well as the affect of general financial measures in the mortgage approval process.

```{r}
newdata <- Mdata[,c(6,9,13,20,24,40,59)]

summary(newdata)


table1 <- table(newdata$`Applicant Race`) 
table2 <- table(newdata$`Self-Employed`)
table3 <- table(newdata$`Marital Status`)

hist(newdata$`Applicant Income`, xlim = c(1,1000000), breaks = 20)
hist(newdata$`Debt to Income Ratio`, xlim = c(1,100), breaks = 30)




```

Notes from different graphs and summary statistics:

Based off "table1", you see in the data set 2041/2380 (86%) of applicants are white. "table2" demonstrates that the majority of applicants are not self-employed, 2103/2380 (88%). The histogram of the Applicants' annual income shows that most of these people fall into a similar ranges of income. 812 applicants fall into the 0-50,000 range, and 1130 applicants fall into the 50,001-100,000 range. It is somewhat intuitive, but it still important to keep in mind that the average applicant is making under 100,000 annually because if the ranges were more evenly spread out across the histogram than the approval prediction would be heavily concerned with the range in which an applicant falls. Instead, the applicants are more or less on an even playing field except for the few outliers making upwards of 500,000. Finally, the histogram of the applicants' Debt to Income Ratio shows the majority of people fall into the 20 to 40 range, which are good ranges in the eyes of the bank as the recognized ideal range is around 28-36. Taking these charts and tables into account, you can see that the average applicants is married, white, employed, making under 100,000 annually and has a solid debt to income ratio in the range of 20 to 40 percent.

```{r}
set.seed(1234)

train <- sample(nrow(newdata), 0.7*nrow(newdata))

newdata.train <- newdata[train,]
newdata.validate <- newdata[-train,]

table(newdata.train$`Action Taken`)
summary(newdata.validate$`Action Taken`)

model <- glm(`Action Taken` ~., data = newdata.train, family = "binomial")

summary(model)

```

Original model set at 0.5

```{r}
prob <- predict(model, newdata.validate, type = 'response')

model.pred <- factor(prob > 0.5, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))

model.perf <- table(newdata.validate$`Action Taken`, model.pred, dnn = c("Actual","Predicted"))

model.perf

my_roc <- roc(newdata.validate$`Action Taken` ~ prob, plot = TRUE)

my_auc <- auc(my_roc)


```

Adjusted model using the threshold value

```{r}
threshold <- my_roc$thresholds[which.max(my_roc$sensitivities + my_roc$specificities)]

model.pred.thresh <- factor(prob > threshold, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))

model.perf.thresh <- table(newdata.validate$`Action Taken`, model.pred.thresh, 
                     dnn = c("Actual","Predicted"))

model.perf.thresh


```


When the threshold was put at 0.5, the model produced a really good number of true approvals, but also yielded 75 false positives (approvals). The threshold taken from the ROC curve was used and produced many more false negatives (predicting denied when actually approved), but limited the false positives (predicting approved when actually denied), reducing them from 75 to 42. Similar to our breast cancer example from class, I think we would prefer the higher number of false negatives rather than false positives, which give applicants unwarranted hope.


Classical Decision Tree

```{r}
set.seed(1234)

dtree <- rpart(`Action Taken` ~., data = newdata.train, method = 'class', parms = list(split = 'information'))

prp(dtree, type = 2, extra = 104,fallen.leaves = T)

dtree.pred <- predict(dtree, newdata.validate,type = "class")

dtree.perf <- table(newdata.validate$`Action Taken`, dtree.pred, dnn = c("Actual","Predicted"))

dtree.perf
```

The classical decision tree model is very similar to the original model. It produced a high number of actual approvals, but also had 77 false approvals. 


Pruned Decision Tree

```{r}
plotcp(dtree)

dtree$cptable

dtree.pruned <- prune(dtree,cp = 0.018)

prp(dtree.pruned, type = 2, extra = 104,fallen.leaves = T)

dtree.pruned.pred <- predict(dtree.pruned, newdata.validate,type = "class")

dtree.pruned.perf <- table(newdata.validate$`Action Taken`, dtree.pruned.pred, dnn = c("Actual","Predicted"))

dtree.pruned.perf
```

Once again with the pruned decision tree, 3 out of the 4 sections were accurate, especially true approvals were the highest they have been (620). However, the false approvals were also at a high compared to other models with 82. As we discussed earlier, we are looking for a model that reducing false approvals in order to limit giving applicants unwarranted confidence in the mortgage process. The small increase in accurate approvals is probably not enough to use this model as the best one.


Support Vector Machine

```{r}
set.seed(1234)

fit.svm <- svm(`Action Taken` ~., data=newdata.train)

fit.svm

svm.pred <- predict(fit.svm,na.omit(newdata.validate))

svm.perf <- table(na.omit(newdata.validate)$`Action Taken`, svm.pred, dnn = c("Actual","Predicted"))

svm.perf

```

The support vector machine produced an almost equivalent number of false approvals, but it also resulted in 0 false denials which is a definite improvement compared to the other models tested.


While evaluating the different models, it is apparent that the inaccurate predicted approvals is something that will always be there. Therefore, I would select the SVM model as it has the second highest actual approvals and also brought the false denials to zero.


```{r}
set.seed(1234)

fit.svm.full <- svm(`Action Taken` ~., data=newdata)

fit.svm.full

svm.pred.full <- predict(fit.svm.full,na.omit(newdata))

svm.perf.full <- table(na.omit(newdata)$`Action Taken`, svm.pred.full, dnn = c("Actual","Predicted"))

svm.perf.full


#my_roc.full <- roc(newdata$`Action Taken` ~ svm.pred.full, plot = TRUE)

#my_auc.full <- auc(my_roc.full)

#Tried all different variations but could not get the ROC curve to work for this model.
```

Report:

In the first couple of sections, I simply was setting up the data and performing the basic logistic regression on the data. Splitting the data into train and validate allows us to do just that, train the different models and the test the model's prediction accuracy using the validate data. I tested our different prediction methods with the training data, and it resulted with the following tables.

```{r}
model.perf
model.perf.thresh
dtree.perf
dtree.pruned.perf
svm.perf

svm.perf.full
```

The results above demonstrate the difference between the actual and predicted outcomes. The numbers in the top right and bottom left are the errors as they represent the incorrect approval and denial predictions. As a whole, the models were fairly similar apart from the original model that used the threshold from the ROC curve. I discussed the issues throughout, and it is again apparent when reviewing the results of the final SVM model using the full data sample. The confusion matrix it produced was solid all around, especially when it came to accurate approvals. However, the weakness lies in the false approval, which are scenarios that predict approval for the applicant but actually resulted in a denial. Though the model had the noticeable fault, the correct predictions it was able to produce was substantial, with an accuracy of 86.6% (2061/2380).
